{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "L8HKKhDRFZNh",
        "6flZkpuzHIsj"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erokemwa/Data-Science/blob/main/img2img_stablediffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last update 23/08\n",
        "- Restart with init image\n",
        "- Model from Google Drive Link"
      ],
      "metadata": {
        "id": "FCwoODW4XsxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "L8HKKhDRFZNh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slEHb8V2FpWE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Check that a GPU is connected\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup\n",
        "!git clone https://github.com/CompVis/stable-diffusion.git\n",
        "!pip install omegaconf einops pytorch-lightning transformers kornia -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!pip3 install color-matcher\n",
        "\n",
        "from IPython.display import clear_output \n",
        "clear_output()\n",
        "\n"
      ],
      "metadata": {
        "id": "hbHSoS2EFwuo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reload\n",
        "\n",
        "#@markdown The error is normal, the system has been reloaded to update the new parameters of the installation\n",
        "\n",
        "import os\n",
        "os._exit(00)\n",
        "\n",
        "#after executing this cell notebook will reload, this is normal, just proceed executing cells"
      ],
      "metadata": {
        "id": "bsG5KS2gF9kQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IogqYvUjY4wW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Download (Choose the method)"
      ],
      "metadata": {
        "id": "0pa176wfY8Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#@title Download model 1.4 from your Google Drive link (optional)\n",
        "\n",
        "#@markdown Copy only your Google Drive link **key**\n",
        "\n",
        "#@markdown Exemple : drive.google.com/file/d/**18n2HARwX_sZuEZbBzJsPDKudeUy_IFbH**/view?usp=sharing\n",
        "\n",
        "#@markdown Your file should be named sd-v1-4.ckpt\n",
        "\n",
        "KeyGDrivelink = \"\" #@param {type:\"string\"}\n",
        "\n",
        "my_file_id = KeyGDrivelink\n",
        "\n",
        "!mkdir -p /content/stable-diffusion/models/ldm/stable-diffusion-v1/\n",
        "%cd /content/stable-diffusion/models/ldm/stable-diffusion-v1\n",
        "!gdown $my_file_id\n",
        "os.rename(\"sd-v1-4.ckpt\", \"model.ckpt\")\n",
        "\n",
        "\n",
        "from IPython.display import clear_output \n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xsM02XkU5S9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Or copy model 1.4 from you Google drive space (optional)\n",
        "# skip the cells from running\n",
        "\n",
        "#@markdown This connects your Google Drive storage and copies the model file. You should already have the model file on your Google Drive storage.\n",
        "#@markdown The sd-v1-4.ckpt file must be at the root of your Google Drive storage space.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/stable-diffusion/models/ldm/stable-diffusion-v1/\n",
        "!cp -r /content/drive/MyDrive/sd-v1-4.ckpt /content/stable-diffusion/models/ldm/stable-diffusion-v1/model.ckpt"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Vk2JYt1b1EXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#@title Or download model (old version 1.3)\n",
        "!mkdir -p /content/stable-diffusion/models/ldm/stable-diffusion-v1/\n",
        "%cd /content/stable-diffusion/models/ldm/stable-diffusion-v1\n",
        "!wget https://drinkordiecdn.lol/sd-v1-3-full-ema.ckpt\n",
        "os.rename(\"sd-v1-3-full-ema.ckpt\", \"model.ckpt\")\n",
        "\n",
        "from IPython.display import clear_output \n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eHYV4nKaPPl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cwBQCkvJZEu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The environment has been reloaded, continue the execution of the following "
      ],
      "metadata": {
        "id": "6flZkpuzHIsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initial configuration\n",
        "%cd /content/stable-diffusion\n",
        "!mkdir ImageC\n",
        "!mkdir MatchImg\n",
        "from IPython.display import clear_output \n",
        "clear_output()"
      ],
      "metadata": {
        "id": "VHJx5wFGGAJs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Importing libraries and defining functions\n",
        "\"\"\"make variations of input image\"\"\"\n",
        "\n",
        "import os\n",
        "import PIL\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange, repeat\n",
        "from torchvision.utils import make_grid\n",
        "from torch import autocast\n",
        "from contextlib import nullcontext\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "\n",
        "def load_model_from_config(config_path = \"configs/stable-diffusion/v1-inference.yaml\", ckpt = \"models/ldm/stable-diffusion-v1/model.ckpt\", verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "\n",
        "    config = OmegaConf.load(config_path)\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_img(path):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "\n",
        "def image2image(prompt, plms = True, outdir = \"/content/output\", n_samples = 3, n_rows = 0, skip_save = False, skip_grid = False, ddim_steps = 50, from_file = None, fixed_code = False, strength = 0.75, init_img = \"/content/stable-diffusion/assets/stable-samples/img2img/sketch-mountains-input.jpg\", C = 4, H = 512, W = 512, f = 8, precision = \"full\", n_iter = 2, seed = 1610684295, scale = 7.5, ddim_eta = 0):\n",
        "\n",
        "\n",
        "    if plms:\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    outpath = outdir\n",
        "\n",
        "    batch_size = n_samples\n",
        "    n_rows = n_rows if n_rows > 0 else batch_size\n",
        "    if not from_file:\n",
        "        prompt = prompt\n",
        "        assert prompt is not None\n",
        "        data = [batch_size * [prompt]]\n",
        "\n",
        "    else:\n",
        "        print(f\"reading prompts from {from_file}\")\n",
        "        with open(from_file, \"r\") as f:\n",
        "            data = f.read().splitlines()\n",
        "            data = list(chunk(data, batch_size))\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    base_count = len(os.listdir(sample_path))\n",
        "    grid_count = len(os.listdir(outpath)) - 1\n",
        "\n",
        "    assert os.path.isfile(init_img)\n",
        "    init_image = load_img(init_img).to(device)\n",
        "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
        "\n",
        "    assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "    t_enc = int(strength * ddim_steps)\n",
        "    print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "    precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                tic = time.time()\n",
        "                all_samples = list()\n",
        "                for n in trange(n_iter, desc=\"Sampling\"):\n",
        "                    for prompts in tqdm(data, desc=\"data\"):\n",
        "                        uc = None\n",
        "                        if scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        # encode (scaled latent)\n",
        "                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                        # decode it\n",
        "                        samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
        "                                                 unconditional_conditioning=uc,)\n",
        "\n",
        "                        x_samples = model.decode_first_stage(samples)\n",
        "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        if not skip_save:\n",
        "                            for x_sample in x_samples:\n",
        "                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                                Image.fromarray(x_sample.astype(np.uint8)).save(\n",
        "                                    os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
        "                                base_count += 1\n",
        "                        all_samples.append(x_samples)\n",
        "\n",
        "                if not skip_grid:\n",
        "                    # additionally, save as grid\n",
        "                    grid = torch.stack(all_samples, 0)\n",
        "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                    grid = make_grid(grid, nrow=n_rows)\n",
        "\n",
        "                    # to image\n",
        "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
        "                    grid_count += 1\n",
        "\n",
        "                toc = time.time()\n",
        "\n",
        "    print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
        "          f\" \\nEnjoy.\")\n",
        "    return grid_count\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "l04JXPFbGEEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loading model\n",
        "model = load_model_from_config()\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "from IPython.display import clear_output \n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_dRNcEo_GFah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All configuration is done, from here use following cells to generate your images"
      ],
      "metadata": {
        "id": "s8-4qmjyHMaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <---  Upload Image\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()           # Use colab upload dialog.\n",
        "uploaded = list(uploaded.keys())    # Get uploaded filenames.\n",
        "assert len(uploaded) == 1           # Make sure only uploaded one file.\n",
        "os.rename(uploaded[0], 'image.png') \n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def resizeAndPad(img, size, padColor=0):\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    sh, sw = size\n",
        "\n",
        "    # interpolation method\n",
        "    if h > sh or w > sw: # shrinking image\n",
        "        interp = cv2.INTER_AREA\n",
        "    else: # stretching image\n",
        "        interp = cv2.INTER_CUBIC\n",
        "\n",
        "    # aspect ratio of image\n",
        "    aspect = w/h  # if on Python 2, you might need to cast as a float: float(w)/h\n",
        "\n",
        "    # compute scaling and pad sizing\n",
        "    if aspect > 1: # horizontal image\n",
        "        new_w = sw\n",
        "        new_h = np.round(new_w/aspect).astype(int)\n",
        "        pad_vert = (sh-new_h)/2\n",
        "        pad_top, pad_bot = np.floor(pad_vert).astype(int), np.ceil(pad_vert).astype(int)\n",
        "        pad_left, pad_right = 0, 0\n",
        "    elif aspect < 1: # vertical image\n",
        "        new_h = sh\n",
        "        new_w = np.round(new_h*aspect).astype(int)\n",
        "        pad_horz = (sw-new_w)/2\n",
        "        pad_left, pad_right = np.floor(pad_horz).astype(int), np.ceil(pad_horz).astype(int)\n",
        "        pad_top, pad_bot = 0, 0\n",
        "    else: # square image\n",
        "        new_h, new_w = sh, sw\n",
        "        pad_left, pad_right, pad_top, pad_bot = 0, 0, 0, 0\n",
        "\n",
        "    # set pad color\n",
        "    if len(img.shape) is 3 and not isinstance(padColor, (list, tuple, np.ndarray)): # color image but only one color provided\n",
        "        padColor = [padColor]*3\n",
        "\n",
        "    # scale and pad\n",
        "    scaled_img = cv2.resize(img, (new_w, new_h), interpolation=interp)\n",
        "    scaled_img = cv2.copyMakeBorder(scaled_img, pad_top, pad_bot, pad_left, pad_right, borderType=cv2.BORDER_CONSTANT, value=padColor)\n",
        "\n",
        "    return scaled_img\n",
        "\n",
        "#@markdown Tesla P100 Max = 704 and Tesla T4 Max = 640\n",
        "maxsize = 640 #@param [\"512\", \"640\", \"704\"] {type:\"raw\"}\n",
        "\n",
        "v_img = cv2.imread('/content/stable-diffusion/image.png') # vertical image\n",
        "scaled_v_img = resizeAndPad(v_img, (maxsize,maxsize), 127)\n",
        "\n",
        "h_img = cv2.imread('/content/stable-diffusion/image.png') # horizontal image\n",
        "scaled_h_img = resizeAndPad(h_img, (maxsize,maxsize), 127)\n",
        "\n",
        "sq_img = cv2.imread('/content/stable-diffusion/image.png') # square image\n",
        "scaled_sq_img = resizeAndPad(sq_img, (maxsize,maxsize), 127)\n",
        "\n",
        "\n",
        "Horizontal_Picture = scaled_h_img\n",
        "Vertical_Picture = scaled_v_img\n",
        "Crop_Picture = scaled_sq_img\n",
        "\n",
        "\n",
        "val = Horizontal_Picture\n",
        "\n",
        "cv2.imwrite('/content/stable-diffusion/ImageC/image_1.png', val)\n",
        "cv2.imwrite('/content/stable-diffusion/MatchImg/matchimg.jpg', val)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GybhWqea_IAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <----- Apply settings and generate\n",
        "\n",
        "#@markdown Don't use short and simple prompts, complex prompts will give better results\n",
        "prompt = \"Portrait, pathtracing, raytracing fantasy, realism, render octane render, 4k\" #@param {type:\"string\"}\n",
        "StartImage = \"/content/stable-diffusion/ImageC/image_1.png\"\n",
        "#@markdown The higher it is, the more the image will be modified [0.1 to 1]\n",
        "Strength = 0.254 #@param {type:\"slider\", min:0, max:1, step:0.001}\n",
        "\n",
        "#@markdown The more steps you use, the better image you get, but I don't recommend using more than 150 steps\n",
        "steps = 68 #@param {type:\"slider\", min:1, max:150, step:1}\n",
        "\n",
        "\n",
        "Height = 512\n",
        "Width = 512\n",
        "\n",
        "\n",
        "#@markdown Setting\n",
        "Samples = 1 #@param [\"1\", \"2\", \"3\", \"4\"] {type:\"raw\"}\n",
        "Iteration = 1 #@param [\"1\", \"2\", \"3\", \"4\"] {type:\"raw\"}\n",
        "Seed = 0 #@param {type:\"slider\", min:0, max:9999999999, step:1}\n",
        "CFGScale = 14.6 #@param {type:\"slider\", min:-2, max:20, step:0.1}\n",
        "\n",
        "\n",
        "sampler = \"ddim\"\n",
        "\n",
        "if sampler == \"plms\":\n",
        "    plms = True\n",
        "else:\n",
        "    plms = False\n",
        "\n",
        "#@title <---- Start generator\n",
        "grid_count = image2image(prompt = prompt, init_img = StartImage, strength = Strength, ddim_steps = steps, plms = plms, H = Height, W = Width, n_samples = Samples, n_iter = Iteration, seed = Seed, scale = CFGScale,)\n",
        "from IPython.display import clear_output \n",
        "clear_output()\n",
        "\n",
        "#@title Result\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = cv2.imread(f\"/content/output/grid-{grid_count-1:04}.png\")\n",
        "\n",
        "cv2_imshow(img)    "
      ],
      "metadata": {
        "id": "SZ1s_qlGJxPy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Recycle Result\n",
        "from color_matcher import ColorMatcher\n",
        "from color_matcher.io_handler import load_img_file, save_img_file, FILE_EXTS\n",
        "from color_matcher.normalizer import Normalizer\n",
        "import os\n",
        "\n",
        "img_ref = load_img_file('/content/stable-diffusion/MatchImg/matchimg.jpg')\n",
        "\n",
        "src_path = '/content/output/'\n",
        "filenames = [os.path.join(src_path, f\"/content/output/grid-{grid_count-1:04}.png\")]\n",
        "\n",
        "cm = ColorMatcher()\n",
        "for i, fname in enumerate(filenames):\n",
        "    img_src = load_img_file(fname)\n",
        "    img_res = cm.transfer(src=img_src, ref=img_ref, method='mkl')\n",
        "    img_res = Normalizer(img_res).uint8_norm()\n",
        "    save_img_file(img_res, os.path.join(os.path.dirname(fname), f\"/content/output/grid-{grid_count-1:04}.png\"))\n",
        "\n",
        "#@markdown This prepares the result to be generated, with a \"Strength\" value not too high, it allows to deeply modify the image while preserving the coherence of the starting image.\n",
        "\n",
        "old_file_name = f\"/content/output/grid-{grid_count-1:04}.png\"\n",
        "new_file_name = \"/content/stable-diffusion/ImageC/image_1.png\"\n",
        "\n",
        "os.rename(old_file_name, new_file_name)\n",
        "\n",
        "print(\"The result is ready to be recycled\")"
      ],
      "metadata": {
        "id": "kJ2Fvex6Kvcy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Restart with init image\n",
        "import shutil\n",
        "import os\n",
        "src = r\"/content/stable-diffusion/MatchImg/matchimg.jpg\"\n",
        "dst = r\"/content/stable-diffusion/ImageC/image_1.png\"\n",
        "shutil.copyfile(src, dst)        "
      ],
      "metadata": {
        "cellView": "form",
        "id": "M28yOoXaOhVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gray edge remover (optional)\n",
        "from PIL import Image, ImageChops\n",
        "\n",
        "def trim(im):\n",
        "    bg = Image.new(\"RGB\", im.size, im.getpixel((0,0)))\n",
        "    diff = ImageChops.difference(im.convert(\"RGB\"), bg)\n",
        "    diff = ImageChops.add(diff, diff, 1.0, -100)\n",
        "    bbox = diff.getbbox()\n",
        "    if bbox:\n",
        "        return im.crop(bbox)\n",
        "\n",
        "im = Image.open(f\"/content/output/grid-{grid_count-1:04}.png\")\n",
        "im = trim(im)\n",
        "im\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XX8htro9qUK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}